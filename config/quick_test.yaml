run:
  stages:
    - inference
    - evaluation
    - analysis

inference:
  models:
    - name: "R1"
      temperature: 1.0
      include_reasoning: true

  datasets:
    - name: "chembench"
      num_questions: 10
      samples_per_question: 3
      shuffle: false

  concurrency:
    max_workers: 50

evaluation:
  grader_model: "gpt-4o"
  max_workers: 50
  grade_legibility: true
  grade_correctness: true
  grade_legibility_chunks: true

analysis:
  baseline_file: "runs/20251013_001335_claude-3-7-sonnet-latest_gpqa/evaluation.json"
  plots:
    - legibility_scores_boxplot
    - correctness_assessment
    - legibility_by_correctness
    - length_vs_legibility
    - correctness_vs_legibility_scatter
    - correctness_vs_legibility_scatter_normalized
    - legibility_by_difficulty
    - legibility_progression
